Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2022-01-08 01:38:49,947] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
Max length: 62
Using amp half precision backend
[2022-01-08 01:39:12,781] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.9+d0ab722, git-hash=d0ab722, git-branch=master
[2022-01-08 01:39:12,789] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups
[2022-01-08 01:39:12,789] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1
[2022-01-08 01:39:12,790] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1
[2022-01-08 01:39:12,790] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]
[2022-01-08 01:39:12,790] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]
[2022-01-08 01:39:12,818] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2022-01-08 01:39:12,994] [INFO] [engine.py:1107:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2022-01-08 01:39:13,010] [INFO] [engine.py:1115:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2022-01-08 01:39:13,010] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2022-01-08 01:39:13,010] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2022-01-08 01:39:13,010] [INFO] [stage_1_and_2.py:113:__init__] Reduce bucket size 500000000
[2022-01-08 01:39:13,010] [INFO] [stage_1_and_2.py:114:__init__] Allgather bucket size 500000000.0
[2022-01-08 01:39:13,010] [INFO] [stage_1_and_2.py:115:__init__] CPU Offload: True
[2022-01-08 01:39:13,010] [INFO] [stage_1_and_2.py:116:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(2651312640, False)]
[2022-01-08 01:39:18,093] [INFO] [utils.py:822:see_memory_usage] Before initializing optimizer states
[2022-01-08 01:39:18,093] [INFO] [utils.py:823:see_memory_usage] MA 5.3 GB         Max_MA 10.61 GB         CA 15.56 GB         Max_CA 16 GB
[2022-01-08 01:39:18,093] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 23.39 GB, percent = 18.6%
[2022-01-08 01:39:24,336] [INFO] [utils.py:822:see_memory_usage] After initializing optimizer states
[2022-01-08 01:39:24,336] [INFO] [utils.py:823:see_memory_usage] MA 5.3 GB         Max_MA 5.3 GB         CA 15.56 GB         Max_CA 16 GB
[2022-01-08 01:39:24,336] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 53.27 GB, percent = 42.4%
[2022-01-08 01:39:24,336] [INFO] [stage_1_and_2.py:483:__init__] optimizer state initialized
[2022-01-08 01:39:24,354] [INFO] [utils.py:822:see_memory_usage] After initializing ZeRO optimizer
[2022-01-08 01:39:24,354] [INFO] [utils.py:823:see_memory_usage] MA 5.3 GB         Max_MA 5.3 GB         CA 15.56 GB         Max_CA 16 GB
[2022-01-08 01:39:24,354] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 53.27 GB, percent = 42.4%
[2022-01-08 01:39:24,354] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2022-01-08 01:39:24,354] [INFO] [engine.py:797:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR
[2022-01-08 01:39:24,354] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fdc2afb6580>
[2022-01-08 01:39:24,354] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:39:24,355] [INFO] [config.py:1058:print] DeepSpeedEngine configuration:
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   amp_enabled .................. False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   amp_params ................... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": null,
    "exps_dir": null,
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   bfloat16_enabled ............. False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   checkpoint_tag_validation_enabled  True
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   checkpoint_tag_validation_fail  False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   communication_data_type ...... None
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   curriculum_enabled ........... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   curriculum_params ............ False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   dataloader_drop_last ......... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   disable_allgather ............ False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   dump_state ................... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_enabled ........... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_gas_boundary_resolution  1
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_layer_num ......... 0
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_max_iter .......... 100
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_stability ......... 1e-06
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_tol ............... 0.01
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   eigenvalue_verbose ........... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   elasticity_enabled ........... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   flops_profiler_config ........ {
    "enabled": false,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   fp16_enabled ................. True
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   fp16_master_weights_and_gradients  False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   fp16_mixed_quantize .......... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   global_rank .................. 0
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   gradient_accumulation_steps .. 1
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   gradient_clipping ............ 0.0
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   gradient_predivide_factor .... 1.0
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   initial_dynamic_scale ........ 4294967296
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   loss_scale ................... 0
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   memory_breakdown ............. False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   optimizer_legacy_fusion ...... False
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   optimizer_name ............... adamw
[2022-01-08 01:39:24,355] [INFO] [config.py:1062:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   pld_enabled .................. False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   pld_params ................... False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   prescale_gradients ........... False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_change_rate ......... 0.001
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_groups .............. 1
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_offset .............. 1000
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_period .............. 1000
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_rounding ............ 0
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_start_bits .......... 16
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_target_bits ......... 8
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_training_enabled .... False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_type ................ 0
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   quantize_verbose ............. False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   scheduler_name ............... WarmupLR
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 50}
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   sparse_attention ............. None
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   sparse_gradients_enabled ..... False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   steps_per_print .............. 10
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   tensorboard_enabled .......... False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   tensorboard_job_name ......... DeepSpeedJobName
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   tensorboard_output_path ......
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   train_batch_size ............. 15
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   train_micro_batch_size_per_gpu  15
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   use_quantizer_kernel ......... False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   wall_clock_breakdown ......... False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   world_size ................... 1
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   zero_allow_untested_optimizer  False
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   zero_config .................. {
    "stage": 2,
    "contiguous_gradients": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 5.000000e+08,
    "allgather_partitions": true,
    "allgather_bucket_size": 5.000000e+08,
    "overlap_comm": false,
    "load_from_fp32_weights": true,
    "elastic_checkpoint": true,
    "offload_param": {
        "device": "cpu",
        "nvme_path": null,
        "buffer_count": 5,
        "buffer_size": 1.000000e+08,
        "max_in_cpu": 1.000000e+09,
        "pin_memory": false
    },
    "offload_optimizer": {
        "device": "cpu",
        "nvme_path": null,
        "buffer_count": 4,
        "pin_memory": false,
        "pipeline_read": false,
        "pipeline_write": false,
        "fast_init": false,
        "pipeline": false
    },
    "sub_group_size": 1.000000e+09,
    "prefetch_bucket_size": 5.000000e+07,
    "param_persistence_threshold": 1.000000e+05,
    "max_live_parameters": 1.000000e+09,
    "max_reuse_distance": 1.000000e+09,
    "gather_fp16_weights_on_model_save": false,
    "ignore_unused_parameters": true,
    "round_robin_gradients": false,
    "legacy_stage1": false
}
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   zero_enabled ................. True
[2022-01-08 01:39:24,356] [INFO] [config.py:1062:print]   zero_optimization_stage ...... 2
[2022-01-08 01:39:24,356] [INFO] [config.py:1064:print]   json = {
    "train_batch_size": 15,
    "fp16": {
        "enabled": true,
        "min_loss_scale": 1,
        "opt_level": "O2"
    },
    "zero_optimization": {
        "stage": 2,
        "offload_param": {
            "device": "cpu"
        },
        "offload_optimizer": {
            "device": "cpu"
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 5.000000e+08,
        "contiguous_gradients": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 5e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 5e-05,
            "warmup_num_steps": 50
        }
    }
}
***** Running training *****
  Num examples = 7008
  Num Epochs = 5
  Instantaneous batch size per device = 15
  Total train batch size (w. parallel, distributed & accumulation) = 15
  Gradient Accumulation steps = 1
  Total optimization steps = 2013
  0%|          | 1/2013 [00:00<28:48,  1.16it/s][2022-01-08 01:39:25,216] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
[2022-01-08 01:39:26,056] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
  0%|          | 3/2013 [00:02<28:18,  1.18it/s][2022-01-08 01:39:26,898] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
[2022-01-08 01:39:27,745] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
  0%|          | 5/2013 [00:04<28:17,  1.18it/s][2022-01-08 01:39:28,589] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
  0%|          | 6/2013 [00:05<28:15,  1.18it/s][2022-01-08 01:39:29,433] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
[2022-01-08 01:39:30,288] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
  0%|          | 8/2013 [00:06<28:57,  1.15it/s][2022-01-08 01:39:31,194] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
  0%|          | 9/2013 [00:07<29:15,  1.14it/s][2022-01-08 01:39:32,090] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
[2022-01-08 01:39:32,936] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
[2022-01-08 01:39:32,936] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=10, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:39:32,936] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=17.476337057468704
  1%|          | 11/2013 [00:09<28:44,  1.16it/s][2022-01-08 01:39:33,786] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
[2022-01-08 01:39:34,633] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
  1%|          | 13/2013 [00:11<28:27,  1.17it/s][2022-01-08 01:39:35,480] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
[2022-01-08 01:39:36,326] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
  1%|          | 15/2013 [00:12<28:18,  1.18it/s][2022-01-08 01:39:37,173] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
[2022-01-08 01:39:38,017] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
  1%|          | 17/2013 [00:14<28:15,  1.18it/s][2022-01-08 01:39:38,869] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2022-01-08 01:39:39,719] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
  1%|          | 19/2013 [00:16<28:14,  1.18it/s][2022-01-08 01:39:40,569] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2022-01-08 01:39:41,419] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2022-01-08 01:39:41,420] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=20, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:39:41,420] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=17.6068113915733
  1%|          | 21/2013 [00:17<28:14,  1.18it/s][2022-01-08 01:39:42,271] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2022-01-08 01:39:43,120] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
  1%|          | 23/2013 [00:19<28:12,  1.18it/s][2022-01-08 01:39:43,971] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
  1%|          | 24/2013 [00:20<28:10,  1.18it/s][2022-01-08 01:39:44,819] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
  1%|          | 25/2013 [00:21<28:09,  1.18it/s][2022-01-08 01:39:45,670] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2022-01-08 01:39:46,518] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256.0, reducing to 128.0
  1%|▏         | 27/2013 [00:23<28:08,  1.18it/s][2022-01-08 01:39:47,370] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128.0, reducing to 64.0
[2022-01-08 01:39:48,221] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64.0, reducing to 32.0
  1%|▏         | 29/2013 [00:24<28:08,  1.17it/s][2022-01-08 01:39:49,074] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32.0, reducing to 16.0
[2022-01-08 01:39:49,923] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16.0, reducing to 8.0
[2022-01-08 01:39:49,924] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=30, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:39:49,924] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=17.62883249432792
  2%|▏         | 32/2013 [00:30<50:21,  1.53s/it]  [2022-01-08 01:39:54,841] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8.0, reducing to 4.0
  2%|▏         | 33/2013 [00:31<43:39,  1.32s/it][2022-01-08 01:39:55,691] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4.0, reducing to 2.0
[2022-01-08 01:39:56,547] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2.0, reducing to 1.0
  2%|▏         | 36/2013 [00:37<55:42,  1.69s/it]  [2022-01-08 01:40:01,475] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1.0, reducing to 1
  2%|▏         | 39/2013 [00:49<1:47:11,  3.26s/it][2022-01-08 01:40:17,759] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=34, lr=[2.2900676539246968e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:40:17,789] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=11.031298344774584
  2%|▏         | 43/2013 [01:02<1:35:40,  2.91s/it][2022-01-08 01:40:26,799] [INFO] [stage_1_and_2.py:1631:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
  2%|▏         | 49/2013 [01:26<2:09:07,  3.94s/it][2022-01-08 01:40:55,351] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=35, lr=[3.4611890029080124e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:40:55,382] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=8.067930724877757
{'loss': 8.0555, 'learning_rate': 3.4611890029080124e-05, 'epoch': 0.11}
  3%|▎         | 59/2013 [02:07<2:12:53,  4.08s/it][2022-01-08 01:41:36,220] [INFO] [logging.py:69:log_dist] [Rank 0] step=60, skipped=35, lr=[4.1140808993222106e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:41:36,249] [INFO] [timer.py:181:stop] 0/60, SamplesPerSec=6.68789572068323
  3%|▎         | 69/2013 [02:48<2:12:24,  4.09s/it][2022-01-08 01:42:17,068] [INFO] [logging.py:69:log_dist] [Rank 0] step=70, skipped=35, lr=[4.544129797493744e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:42:17,097] [INFO] [timer.py:181:stop] 0/70, SamplesPerSec=5.96801566122986
  4%|▍         | 79/2013 [03:29<2:11:40,  4.09s/it][2022-01-08 01:42:57,921] [INFO] [logging.py:69:log_dist] [Rank 0] step=80, skipped=35, lr=[4.8653375561549195e-05], mom=[[0.9, 0.999]]
  4%|▍         | 80/2013 [03:33<2:11:37,  4.09s/it][2022-01-08 01:42:57,954] [INFO] [timer.py:181:stop] 0/80, SamplesPerSec=5.525526128208344
  4%|▍         | 89/2013 [04:10<2:11:03,  4.09s/it][2022-01-08 01:43:38,801] [INFO] [logging.py:69:log_dist] [Rank 0] step=90, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
  4%|▍         | 90/2013 [04:14<2:10:56,  4.09s/it][2022-01-08 01:43:38,831] [INFO] [timer.py:181:stop] 0/90, SamplesPerSec=5.225669499980795
  5%|▍         | 99/2013 [04:51<2:10:22,  4.09s/it][2022-01-08 01:44:19,655] [INFO] [logging.py:69:log_dist] [Rank 0] step=100, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:44:19,689] [INFO] [timer.py:181:stop] 0/100, SamplesPerSec=5.009645748243033
{'loss': 3.5939, 'learning_rate': 5e-05, 'epoch': 0.21}
  5%|▌         | 109/2013 [05:32<2:11:54,  4.16s/it][2022-01-08 01:45:00,985] [INFO] [logging.py:69:log_dist] [Rank 0] step=110, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
  5%|▌         | 110/2013 [05:36<2:11:34,  4.15s/it][2022-01-08 01:45:01,021] [INFO] [timer.py:181:stop] 0/110, SamplesPerSec=4.8395545003806975
  6%|▌         | 119/2013 [06:13<2:10:51,  4.15s/it][2022-01-08 01:45:42,347] [INFO] [logging.py:69:log_dist] [Rank 0] step=120, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
  6%|▌         | 120/2013 [06:18<2:10:50,  4.15s/it][2022-01-08 01:45:42,389] [INFO] [timer.py:181:stop] 0/120, SamplesPerSec=4.706362155986512
  6%|▋         | 129/2013 [06:54<2:07:29,  4.06s/it][2022-01-08 01:46:23,102] [INFO] [logging.py:69:log_dist] [Rank 0] step=130, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
  6%|▋         | 130/2013 [06:58<2:07:17,  4.06s/it][2022-01-08 01:46:23,132] [INFO] [timer.py:181:stop] 0/130, SamplesPerSec=4.606489989123324
  7%|▋         | 139/2013 [07:35<2:08:37,  4.12s/it][2022-01-08 01:47:04,151] [INFO] [logging.py:69:log_dist] [Rank 0] step=140, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
  7%|▋         | 140/2013 [07:39<2:10:36,  4.18s/it][2022-01-08 01:47:04,181] [INFO] [timer.py:181:stop] 0/140, SamplesPerSec=4.521363810177863
  7%|▋         | 149/2013 [08:16<2:07:19,  4.10s/it][2022-01-08 01:47:45,118] [INFO] [logging.py:69:log_dist] [Rank 0] step=150, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:47:45,148] [INFO] [timer.py:181:stop] 0/150, SamplesPerSec=4.450944835640465
{'loss': 2.9109, 'learning_rate': 5e-05, 'epoch': 0.32}
  8%|▊         | 159/2013 [08:57<2:05:06,  4.05s/it][2022-01-08 01:48:25,604] [INFO] [logging.py:69:log_dist] [Rank 0] step=160, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
  8%|▊         | 160/2013 [09:01<2:05:01,  4.05s/it][2022-01-08 01:48:25,633] [INFO] [timer.py:181:stop] 0/160, SamplesPerSec=4.395125942496921
  8%|▊         | 169/2013 [09:37<2:04:25,  4.05s/it][2022-01-08 01:49:06,091] [INFO] [logging.py:69:log_dist] [Rank 0] step=170, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:49:06,121] [INFO] [timer.py:181:stop] 0/170, SamplesPerSec=4.347081921578749
  9%|▉         | 179/2013 [10:18<2:03:42,  4.05s/it][2022-01-08 01:49:46,567] [INFO] [logging.py:69:log_dist] [Rank 0] step=180, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:49:46,596] [INFO] [timer.py:181:stop] 0/180, SamplesPerSec=4.30539080340622
  9%|▉         | 189/2013 [10:58<2:03:01,  4.05s/it][2022-01-08 01:50:27,060] [INFO] [logging.py:69:log_dist] [Rank 0] step=190, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:50:27,094] [INFO] [timer.py:181:stop] 0/190, SamplesPerSec=4.268662845854158
 10%|▉         | 199/2013 [11:39<2:02:24,  4.05s/it][2022-01-08 01:51:07,548] [INFO] [logging.py:69:log_dist] [Rank 0] step=200, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:51:07,578] [INFO] [timer.py:181:stop] 0/200, SamplesPerSec=4.236264567109609
{'loss': 2.5181, 'learning_rate': 5e-05, 'epoch': 0.43}
 10%|█         | 209/2013 [12:19<2:01:39,  4.05s/it][2022-01-08 01:51:48,014] [INFO] [logging.py:69:log_dist] [Rank 0] step=210, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 10%|█         | 210/2013 [12:23<2:01:34,  4.05s/it][2022-01-08 01:51:48,043] [INFO] [timer.py:181:stop] 0/210, SamplesPerSec=4.2075055963236165
 11%|█         | 219/2013 [13:00<2:00:59,  4.05s/it][2022-01-08 01:52:28,480] [INFO] [logging.py:69:log_dist] [Rank 0] step=220, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 11%|█         | 220/2013 [13:04<2:00:53,  4.05s/it][2022-01-08 01:52:28,509] [INFO] [timer.py:181:stop] 0/220, SamplesPerSec=4.181716400622012
 11%|█▏        | 229/2013 [13:40<2:00:21,  4.05s/it][2022-01-08 01:53:08,956] [INFO] [logging.py:69:log_dist] [Rank 0] step=230, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:53:08,985] [INFO] [timer.py:181:stop] 0/230, SamplesPerSec=4.15841161249685
 12%|█▏        | 239/2013 [14:21<1:59:40,  4.05s/it][2022-01-08 01:53:49,436] [INFO] [logging.py:69:log_dist] [Rank 0] step=240, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 12%|█▏        | 240/2013 [14:25<1:59:35,  4.05s/it][2022-01-08 01:53:49,465] [INFO] [timer.py:181:stop] 0/240, SamplesPerSec=4.1372710498922505
 12%|█▏        | 249/2013 [15:01<1:59:01,  4.05s/it][2022-01-08 01:54:29,916] [INFO] [logging.py:69:log_dist] [Rank 0] step=250, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:54:29,945] [INFO] [timer.py:181:stop] 0/250, SamplesPerSec=4.118029453304923
{'loss': 2.2938, 'learning_rate': 5e-05, 'epoch': 0.53}
 13%|█▎        | 259/2013 [15:42<1:58:19,  4.05s/it][2022-01-08 01:55:10,398] [INFO] [logging.py:69:log_dist] [Rank 0] step=260, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 13%|█▎        | 260/2013 [15:46<1:58:14,  4.05s/it][2022-01-08 01:55:10,427] [INFO] [timer.py:181:stop] 0/260, SamplesPerSec=4.100428112215623
 13%|█▎        | 269/2013 [16:22<1:57:37,  4.05s/it][2022-01-08 01:55:50,879] [INFO] [logging.py:69:log_dist] [Rank 0] step=270, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 13%|█▎        | 270/2013 [16:26<1:57:34,  4.05s/it][2022-01-08 01:55:50,908] [INFO] [timer.py:181:stop] 0/270, SamplesPerSec=4.08427563519003
 14%|█▍        | 279/2013 [17:02<1:56:57,  4.05s/it][2022-01-08 01:56:31,354] [INFO] [logging.py:69:log_dist] [Rank 0] step=280, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:56:31,383] [INFO] [timer.py:181:stop] 0/280, SamplesPerSec=4.069423588039075
 14%|█▍        | 289/2013 [17:43<1:56:15,  4.05s/it][2022-01-08 01:57:11,823] [INFO] [logging.py:69:log_dist] [Rank 0] step=290, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:57:11,853] [INFO] [timer.py:181:stop] 0/290, SamplesPerSec=4.055719991558985
 15%|█▍        | 299/2013 [18:23<1:55:35,  4.05s/it][2022-01-08 01:57:52,286] [INFO] [logging.py:69:log_dist] [Rank 0] step=300, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 15%|█▍        | 300/2013 [18:27<1:55:30,  4.05s/it][2022-01-08 01:57:52,316] [INFO] [timer.py:181:stop] 0/300, SamplesPerSec=4.043043675522679
{'loss': 2.2055, 'learning_rate': 5e-05, 'epoch': 0.64}
 15%|█▌        | 309/2013 [19:04<1:54:54,  4.05s/it][2022-01-08 01:58:32,751] [INFO] [logging.py:69:log_dist] [Rank 0] step=310, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:58:32,780] [INFO] [timer.py:181:stop] 0/310, SamplesPerSec=4.031256843267691
 16%|█▌        | 319/2013 [19:44<1:54:15,  4.05s/it][2022-01-08 01:59:13,224] [INFO] [logging.py:69:log_dist] [Rank 0] step=320, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:59:13,254] [INFO] [timer.py:181:stop] 0/320, SamplesPerSec=4.020242988154115
 16%|█▋        | 329/2013 [20:25<1:53:32,  4.05s/it][2022-01-08 01:59:53,684] [INFO] [logging.py:69:log_dist] [Rank 0] step=330, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 01:59:53,713] [INFO] [timer.py:181:stop] 0/330, SamplesPerSec=4.010001540923997
 17%|█▋        | 339/2013 [21:05<1:52:53,  4.05s/it][2022-01-08 02:00:34,158] [INFO] [logging.py:69:log_dist] [Rank 0] step=340, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 17%|█▋        | 340/2013 [21:09<1:52:49,  4.05s/it][2022-01-08 02:00:34,187] [INFO] [timer.py:181:stop] 0/340, SamplesPerSec=4.000367521535013
 17%|█▋        | 349/2013 [21:46<1:52:14,  4.05s/it][2022-01-08 02:01:14,624] [INFO] [logging.py:69:log_dist] [Rank 0] step=350, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:01:14,653] [INFO] [timer.py:181:stop] 0/350, SamplesPerSec=3.9913515055541375
{'loss': 2.1103, 'learning_rate': 5e-05, 'epoch': 0.75}
 18%|█▊        | 359/2013 [22:26<1:51:37,  4.05s/it][2022-01-08 02:01:55,112] [INFO] [logging.py:69:log_dist] [Rank 0] step=360, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:01:55,141] [INFO] [timer.py:181:stop] 0/360, SamplesPerSec=3.982814524697442
 18%|█▊        | 369/2013 [23:07<1:50:52,  4.05s/it][2022-01-08 02:02:35,591] [INFO] [logging.py:69:log_dist] [Rank 0] step=370, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:02:35,620] [INFO] [timer.py:181:stop] 0/370, SamplesPerSec=3.9747992600976216
 19%|█▉        | 379/2013 [23:47<1:50:11,  4.05s/it][2022-01-08 02:03:16,067] [INFO] [logging.py:69:log_dist] [Rank 0] step=380, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 19%|█▉        | 380/2013 [23:51<1:50:07,  4.05s/it][2022-01-08 02:03:16,096] [INFO] [timer.py:181:stop] 0/380, SamplesPerSec=3.967245180913496
 19%|█▉        | 389/2013 [24:28<1:49:31,  4.05s/it][2022-01-08 02:03:56,529] [INFO] [logging.py:69:log_dist] [Rank 0] step=390, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:03:56,558] [INFO] [timer.py:181:stop] 0/390, SamplesPerSec=3.960144396097978
 20%|█▉        | 399/2013 [25:08<1:48:48,  4.04s/it][2022-01-08 02:04:36,990] [INFO] [logging.py:69:log_dist] [Rank 0] step=400, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:04:37,019] [INFO] [timer.py:181:stop] 0/400, SamplesPerSec=3.953427795728032
{'loss': 2.0659, 'learning_rate': 5e-05, 'epoch': 0.85}
 20%|██        | 409/2013 [25:49<1:49:40,  4.10s/it][2022-01-08 02:05:17,860] [INFO] [logging.py:69:log_dist] [Rank 0] step=410, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:05:17,889] [INFO] [timer.py:181:stop] 0/410, SamplesPerSec=3.946022334146505
 21%|██        | 419/2013 [26:30<1:48:24,  4.08s/it][2022-01-08 02:05:58,738] [INFO] [logging.py:69:log_dist] [Rank 0] step=420, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:05:58,767] [INFO] [timer.py:181:stop] 0/420, SamplesPerSec=3.938975216922789
 21%|██▏       | 429/2013 [27:11<1:47:11,  4.06s/it][2022-01-08 02:06:39,470] [INFO] [logging.py:69:log_dist] [Rank 0] step=430, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 21%|██▏       | 430/2013 [27:15<1:47:00,  4.06s/it][2022-01-08 02:06:39,500] [INFO] [timer.py:181:stop] 0/430, SamplesPerSec=3.9326352521937826
 22%|██▏       | 439/2013 [27:51<1:46:08,  4.05s/it][2022-01-08 02:07:19,928] [INFO] [logging.py:69:log_dist] [Rank 0] step=440, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 22%|██▏       | 440/2013 [27:55<1:46:02,  4.05s/it][2022-01-08 02:07:19,957] [INFO] [timer.py:181:stop] 0/440, SamplesPerSec=3.927246369217363
 22%|██▏       | 449/2013 [28:32<1:45:27,  4.05s/it][2022-01-08 02:08:00,388] [INFO] [logging.py:69:log_dist] [Rank 0] step=450, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 22%|██▏       | 450/2013 [28:36<1:45:22,  4.05s/it][2022-01-08 02:08:00,417] [INFO] [timer.py:181:stop] 0/450, SamplesPerSec=3.922104804486527
{'loss': 2.0305, 'learning_rate': 5e-05, 'epoch': 0.96}
 23%|██▎       | 459/2013 [29:12<1:44:47,  4.05s/it][2022-01-08 02:08:40,842] [INFO] [logging.py:69:log_dist] [Rank 0] step=460, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:08:40,872] [INFO] [timer.py:181:stop] 0/460, SamplesPerSec=3.9172138294729657
 23%|██▎       | 469/2013 [29:52<1:43:18,  4.01s/it][2022-01-08 02:09:21,153] [INFO] [logging.py:69:log_dist] [Rank 0] step=470, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:09:21,182] [INFO] [timer.py:181:stop] 0/470, SamplesPerSec=3.912857902795985
 24%|██▍       | 479/2013 [30:33<1:43:23,  4.04s/it][2022-01-08 02:10:01,600] [INFO] [logging.py:69:log_dist] [Rank 0] step=480, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:10:01,629] [INFO] [timer.py:181:stop] 0/480, SamplesPerSec=3.9084021261057664
 24%|██▍       | 489/2013 [31:13<1:42:48,  4.05s/it][2022-01-08 02:10:42,068] [INFO] [logging.py:69:log_dist] [Rank 0] step=490, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 24%|██▍       | 490/2013 [31:17<1:42:44,  4.05s/it][2022-01-08 02:10:42,098] [INFO] [timer.py:181:stop] 0/490, SamplesPerSec=3.9040916476022516
 25%|██▍       | 499/2013 [31:54<1:42:05,  4.05s/it][2022-01-08 02:11:22,528] [INFO] [logging.py:69:log_dist] [Rank 0] step=500, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 25%|██▍       | 500/2013 [31:58<1:42:00,  4.05s/it][2022-01-08 02:11:22,558] [INFO] [timer.py:181:stop] 0/500, SamplesPerSec=3.8999822731421663
{'loss': 1.7833, 'learning_rate': 5e-05, 'epoch': 1.07}
 25%|██▌       | 509/2013 [32:34<1:41:25,  4.05s/it][2022-01-08 02:12:02,995] [INFO] [logging.py:69:log_dist] [Rank 0] step=510, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:12:03,024] [INFO] [timer.py:181:stop] 0/510, SamplesPerSec=3.8960307883299508
 26%|██▌       | 519/2013 [33:15<1:40:44,  4.05s/it][2022-01-08 02:12:43,455] [INFO] [logging.py:69:log_dist] [Rank 0] step=520, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:12:43,485] [INFO] [timer.py:181:stop] 0/520, SamplesPerSec=3.892250043059626
 26%|██▋       | 529/2013 [33:55<1:40:05,  4.05s/it][2022-01-08 02:13:23,923] [INFO] [logging.py:69:log_dist] [Rank 0] step=530, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:13:23,952] [INFO] [timer.py:181:stop] 0/530, SamplesPerSec=3.8886053812957786
 27%|██▋       | 539/2013 [34:36<1:39:25,  4.05s/it][2022-01-08 02:14:04,387] [INFO] [logging.py:69:log_dist] [Rank 0] step=540, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:14:04,416] [INFO] [timer.py:181:stop] 0/540, SamplesPerSec=3.8851096882859215
 27%|██▋       | 549/2013 [35:16<1:38:44,  4.05s/it][2022-01-08 02:14:44,857] [INFO] [logging.py:69:log_dist] [Rank 0] step=550, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:14:44,886] [INFO] [timer.py:181:stop] 0/550, SamplesPerSec=3.8817375437319637
{'loss': 1.5955, 'learning_rate': 5e-05, 'epoch': 1.18}
 28%|██▊       | 559/2013 [35:56<1:38:03,  4.05s/it][2022-01-08 02:15:25,319] [INFO] [logging.py:69:log_dist] [Rank 0] step=560, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:15:25,348] [INFO] [timer.py:181:stop] 0/560, SamplesPerSec=3.878506793993267
 28%|██▊       | 569/2013 [36:37<1:37:26,  4.05s/it][2022-01-08 02:16:05,801] [INFO] [logging.py:69:log_dist] [Rank 0] step=570, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:16:05,831] [INFO] [timer.py:181:stop] 0/570, SamplesPerSec=3.8753569323112336
 29%|██▉       | 579/2013 [37:17<1:36:42,  4.05s/it][2022-01-08 02:16:46,257] [INFO] [logging.py:69:log_dist] [Rank 0] step=580, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 29%|██▉       | 580/2013 [37:21<1:36:37,  4.05s/it][2022-01-08 02:16:46,286] [INFO] [timer.py:181:stop] 0/580, SamplesPerSec=3.8723671205522794
 29%|██▉       | 589/2013 [37:58<1:36:01,  4.05s/it][2022-01-08 02:17:26,713] [INFO] [logging.py:69:log_dist] [Rank 0] step=590, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:17:26,742] [INFO] [timer.py:181:stop] 0/590, SamplesPerSec=3.869484178092346
 30%|██▉       | 599/2013 [38:38<1:35:20,  4.05s/it][2022-01-08 02:18:07,172] [INFO] [logging.py:69:log_dist] [Rank 0] step=600, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 30%|██▉       | 600/2013 [38:42<1:35:16,  4.05s/it][2022-01-08 02:18:07,201] [INFO] [timer.py:181:stop] 0/600, SamplesPerSec=3.866695607143853
{'loss': 1.6314, 'learning_rate': 5e-05, 'epoch': 1.28}
 30%|███       | 609/2013 [39:19<1:34:41,  4.05s/it][2022-01-08 02:18:47,637] [INFO] [logging.py:69:log_dist] [Rank 0] step=610, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 30%|███       | 610/2013 [39:23<1:34:36,  4.05s/it][2022-01-08 02:18:47,667] [INFO] [timer.py:181:stop] 0/610, SamplesPerSec=3.863995294753564
 31%|███       | 619/2013 [39:59<1:33:59,  4.05s/it][2022-01-08 02:19:28,096] [INFO] [logging.py:69:log_dist] [Rank 0] step=620, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:19:28,126] [INFO] [timer.py:181:stop] 0/620, SamplesPerSec=3.8613944667162796
 31%|███       | 629/2013 [40:40<1:33:18,  4.05s/it][2022-01-08 02:20:08,549] [INFO] [logging.py:69:log_dist] [Rank 0] step=630, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 31%|███▏      | 630/2013 [40:44<1:33:15,  4.05s/it][2022-01-08 02:20:08,578] [INFO] [timer.py:181:stop] 0/630, SamplesPerSec=3.858889805647715
 32%|███▏      | 639/2013 [41:20<1:32:39,  4.05s/it][2022-01-08 02:20:49,010] [INFO] [logging.py:69:log_dist] [Rank 0] step=640, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 32%|███▏      | 640/2013 [41:24<1:32:35,  4.05s/it][2022-01-08 02:20:49,039] [INFO] [timer.py:181:stop] 0/640, SamplesPerSec=3.856452547471229
 32%|███▏      | 649/2013 [42:01<1:31:59,  4.05s/it][2022-01-08 02:21:29,483] [INFO] [logging.py:69:log_dist] [Rank 0] step=650, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:21:29,512] [INFO] [timer.py:181:stop] 0/650, SamplesPerSec=3.854075451842506
{'loss': 1.6386, 'learning_rate': 5e-05, 'epoch': 1.39}
 33%|███▎      | 659/2013 [42:41<1:31:21,  4.05s/it][2022-01-08 02:22:09,966] [INFO] [logging.py:69:log_dist] [Rank 0] step=660, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:22:09,995] [INFO] [timer.py:181:stop] 0/660, SamplesPerSec=3.851759156810231
 33%|███▎      | 669/2013 [43:22<1:30:38,  4.05s/it][2022-01-08 02:22:50,434] [INFO] [logging.py:69:log_dist] [Rank 0] step=670, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:22:50,463] [INFO] [timer.py:181:stop] 0/670, SamplesPerSec=3.8495358080187763
 34%|███▎      | 679/2013 [44:02<1:29:56,  4.05s/it][2022-01-08 02:23:30,888] [INFO] [logging.py:69:log_dist] [Rank 0] step=680, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 34%|███▍      | 680/2013 [44:06<1:29:52,  4.05s/it][2022-01-08 02:23:30,918] [INFO] [timer.py:181:stop] 0/680, SamplesPerSec=3.8474007121496987
 34%|███▍      | 689/2013 [44:42<1:29:16,  4.05s/it][2022-01-08 02:24:11,349] [INFO] [logging.py:69:log_dist] [Rank 0] step=690, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:24:11,379] [INFO] [timer.py:181:stop] 0/690, SamplesPerSec=3.84532033204538
 35%|███▍      | 699/2013 [45:23<1:28:36,  4.05s/it][2022-01-08 02:24:51,815] [INFO] [logging.py:69:log_dist] [Rank 0] step=700, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 35%|███▍      | 700/2013 [45:27<1:28:31,  4.05s/it][2022-01-08 02:24:51,844] [INFO] [timer.py:181:stop] 0/700, SamplesPerSec=3.8432963755921055
{'loss': 1.6634, 'learning_rate': 5e-05, 'epoch': 1.5}
 35%|███▌      | 709/2013 [46:03<1:27:57,  4.05s/it][2022-01-08 02:25:32,284] [INFO] [logging.py:69:log_dist] [Rank 0] step=710, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:25:32,314] [INFO] [timer.py:181:stop] 0/710, SamplesPerSec=3.8413256812881262
 36%|███▌      | 719/2013 [46:44<1:27:17,  4.05s/it][2022-01-08 02:26:12,754] [INFO] [logging.py:69:log_dist] [Rank 0] step=720, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:26:12,784] [INFO] [timer.py:181:stop] 0/720, SamplesPerSec=3.8394109698718157
 36%|███▌      | 729/2013 [47:24<1:26:35,  4.05s/it][2022-01-08 02:26:53,217] [INFO] [logging.py:69:log_dist] [Rank 0] step=730, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:26:53,246] [INFO] [timer.py:181:stop] 0/730, SamplesPerSec=3.8375600621192807
 37%|███▋      | 739/2013 [48:05<1:25:55,  4.05s/it][2022-01-08 02:27:33,680] [INFO] [logging.py:69:log_dist] [Rank 0] step=740, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:27:33,709] [INFO] [timer.py:181:stop] 0/740, SamplesPerSec=3.8357602518968394
 37%|███▋      | 749/2013 [48:45<1:25:14,  4.05s/it][2022-01-08 02:28:14,146] [INFO] [logging.py:69:log_dist] [Rank 0] step=750, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:28:14,175] [INFO] [timer.py:181:stop] 0/750, SamplesPerSec=3.8340076399035885
{'loss': 1.6479, 'learning_rate': 5e-05, 'epoch': 1.6}
 38%|███▊      | 759/2013 [49:26<1:24:34,  4.05s/it][2022-01-08 02:28:54,609] [INFO] [logging.py:69:log_dist] [Rank 0] step=760, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:28:54,638] [INFO] [timer.py:181:stop] 0/760, SamplesPerSec=3.8323056392290065
 38%|███▊      | 769/2013 [50:06<1:23:55,  4.05s/it][2022-01-08 02:29:35,089] [INFO] [logging.py:69:log_dist] [Rank 0] step=770, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:29:35,118] [INFO] [timer.py:181:stop] 0/770, SamplesPerSec=3.830627444246552
 39%|███▊      | 779/2013 [50:47<1:23:14,  4.05s/it][2022-01-08 02:30:15,559] [INFO] [logging.py:69:log_dist] [Rank 0] step=780, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 39%|███▊      | 780/2013 [50:51<1:23:10,  4.05s/it][2022-01-08 02:30:15,589] [INFO] [timer.py:181:stop] 0/780, SamplesPerSec=3.8290056714847913
 39%|███▉      | 789/2013 [51:27<1:22:32,  4.05s/it][2022-01-08 02:30:56,022] [INFO] [logging.py:69:log_dist] [Rank 0] step=790, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 39%|███▉      | 790/2013 [51:31<1:22:29,  4.05s/it][2022-01-08 02:30:56,051] [INFO] [timer.py:181:stop] 0/790, SamplesPerSec=3.827436712396159
 40%|███▉      | 799/2013 [52:08<1:21:51,  4.05s/it][2022-01-08 02:31:36,485] [INFO] [logging.py:69:log_dist] [Rank 0] step=800, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 40%|███▉      | 800/2013 [52:12<1:21:47,  4.05s/it][2022-01-08 02:31:36,514] [INFO] [timer.py:181:stop] 0/800, SamplesPerSec=3.8259080607520604
{'loss': 1.6626, 'learning_rate': 5e-05, 'epoch': 1.71}
 40%|████      | 809/2013 [52:48<1:21:12,  4.05s/it][2022-01-08 02:32:16,957] [INFO] [logging.py:69:log_dist] [Rank 0] step=810, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:32:16,986] [INFO] [timer.py:181:stop] 0/810, SamplesPerSec=3.8244084164060608
 41%|████      | 819/2013 [53:29<1:20:31,  4.05s/it][2022-01-08 02:32:57,424] [INFO] [logging.py:69:log_dist] [Rank 0] step=820, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:32:57,454] [INFO] [timer.py:181:stop] 0/820, SamplesPerSec=3.8229513148389014
 41%|████      | 829/2013 [54:09<1:19:51,  4.05s/it][2022-01-08 02:33:37,885] [INFO] [logging.py:69:log_dist] [Rank 0] step=830, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:33:37,915] [INFO] [timer.py:181:stop] 0/830, SamplesPerSec=3.821537705164049
 42%|████▏     | 839/2013 [54:49<1:19:10,  4.05s/it][2022-01-08 02:34:18,351] [INFO] [logging.py:69:log_dist] [Rank 0] step=840, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 42%|████▏     | 840/2013 [54:54<1:19:06,  4.05s/it][2022-01-08 02:34:18,381] [INFO] [timer.py:181:stop] 0/840, SamplesPerSec=3.820152968516307
 42%|████▏     | 849/2013 [55:30<1:18:30,  4.05s/it][2022-01-08 02:34:58,823] [INFO] [logging.py:69:log_dist] [Rank 0] step=850, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 42%|████▏     | 850/2013 [55:34<1:18:26,  4.05s/it][2022-01-08 02:34:58,852] [INFO] [timer.py:181:stop] 0/850, SamplesPerSec=3.8187951577261847
{'loss': 1.6684, 'learning_rate': 5e-05, 'epoch': 1.82}
 43%|████▎     | 859/2013 [56:10<1:17:50,  4.05s/it][2022-01-08 02:35:39,295] [INFO] [logging.py:69:log_dist] [Rank 0] step=860, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:35:39,325] [INFO] [timer.py:181:stop] 0/860, SamplesPerSec=3.8174698476898015
 43%|████▎     | 869/2013 [56:51<1:17:09,  4.05s/it][2022-01-08 02:36:19,765] [INFO] [logging.py:69:log_dist] [Rank 0] step=870, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:36:19,795] [INFO] [timer.py:181:stop] 0/870, SamplesPerSec=3.816178815977781
 44%|████▎     | 879/2013 [57:31<1:16:29,  4.05s/it][2022-01-08 02:37:00,236] [INFO] [logging.py:69:log_dist] [Rank 0] step=880, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:37:00,265] [INFO] [timer.py:181:stop] 0/880, SamplesPerSec=3.8149162260617087
 44%|████▍     | 889/2013 [58:12<1:15:47,  4.05s/it][2022-01-08 02:37:40,697] [INFO] [logging.py:69:log_dist] [Rank 0] step=890, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:37:40,727] [INFO] [timer.py:181:stop] 0/890, SamplesPerSec=3.8136928093326032
 45%|████▍     | 899/2013 [58:52<1:15:07,  4.05s/it][2022-01-08 02:38:21,159] [INFO] [logging.py:69:log_dist] [Rank 0] step=900, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:38:21,188] [INFO] [timer.py:181:stop] 0/900, SamplesPerSec=3.8124973793214734
{'loss': 1.6559, 'learning_rate': 5e-05, 'epoch': 1.92}
 45%|████▌     | 909/2013 [59:33<1:14:26,  4.05s/it][2022-01-08 02:39:01,612] [INFO] [logging.py:69:log_dist] [Rank 0] step=910, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:39:01,641] [INFO] [timer.py:181:stop] 0/910, SamplesPerSec=3.811339152155201
 46%|████▌     | 919/2013 [1:00:13<1:13:46,  4.05s/it][2022-01-08 02:39:42,075] [INFO] [logging.py:69:log_dist] [Rank 0] step=920, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 46%|████▌     | 920/2013 [1:00:17<1:13:42,  4.05s/it][2022-01-08 02:39:42,105] [INFO] [timer.py:181:stop] 0/920, SamplesPerSec=3.8101944253176074
 46%|████▌     | 929/2013 [1:00:54<1:13:05,  4.05s/it][2022-01-08 02:40:22,530] [INFO] [logging.py:69:log_dist] [Rank 0] step=930, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:40:22,559] [INFO] [timer.py:181:stop] 0/930, SamplesPerSec=3.809085397484642
 47%|████▋     | 939/2013 [1:01:34<1:12:05,  4.03s/it][2022-01-08 02:41:02,820] [INFO] [logging.py:69:log_dist] [Rank 0] step=940, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:41:02,849] [INFO] [timer.py:181:stop] 0/940, SamplesPerSec=3.8081701776944348
 47%|████▋     | 949/2013 [1:02:14<1:11:44,  4.05s/it][2022-01-08 02:41:43,278] [INFO] [logging.py:69:log_dist] [Rank 0] step=950, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 47%|████▋     | 950/2013 [1:02:18<1:11:40,  4.05s/it][2022-01-08 02:41:43,309] [INFO] [timer.py:181:stop] 0/950, SamplesPerSec=3.8071008520815415
{'loss': 1.5356, 'learning_rate': 5e-05, 'epoch': 2.03}
 48%|████▊     | 959/2013 [1:02:55<1:11:04,  4.05s/it][2022-01-08 02:42:23,743] [INFO] [logging.py:69:log_dist] [Rank 0] step=960, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:42:23,772] [INFO] [timer.py:181:stop] 0/960, SamplesPerSec=3.8060521384459487
 48%|████▊     | 969/2013 [1:03:35<1:10:23,  4.05s/it][2022-01-08 02:43:04,195] [INFO] [logging.py:69:log_dist] [Rank 0] step=970, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:43:04,225] [INFO] [timer.py:181:stop] 0/970, SamplesPerSec=3.805035727408843
 49%|████▊     | 979/2013 [1:04:16<1:09:43,  4.05s/it][2022-01-08 02:43:44,657] [INFO] [logging.py:69:log_dist] [Rank 0] step=980, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:43:44,686] [INFO] [timer.py:181:stop] 0/980, SamplesPerSec=3.8040317669755774
 49%|████▉     | 989/2013 [1:04:56<1:09:03,  4.05s/it][2022-01-08 02:44:25,116] [INFO] [logging.py:69:log_dist] [Rank 0] step=990, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 49%|████▉     | 990/2013 [1:05:00<1:08:59,  4.05s/it][2022-01-08 02:44:25,145] [INFO] [timer.py:181:stop] 0/990, SamplesPerSec=3.8030511553031
 50%|████▉     | 999/2013 [1:05:37<1:08:22,  4.05s/it][2022-01-08 02:45:05,572] [INFO] [logging.py:69:log_dist] [Rank 0] step=1000, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 50%|████▉     | 1000/2013 [1:05:41<1:08:17,  4.05s/it][2022-01-08 02:45:05,602] [INFO] [timer.py:181:stop] 0/1000, SamplesPerSec=3.8020936727615156
{'loss': 1.1188, 'learning_rate': 5e-05, 'epoch': 2.14}
 50%|█████     | 1009/2013 [1:06:17<1:07:42,  4.05s/it][2022-01-08 02:45:46,035] [INFO] [logging.py:69:log_dist] [Rank 0] step=1010, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 50%|█████     | 1010/2013 [1:06:21<1:07:38,  4.05s/it][2022-01-08 02:45:46,064] [INFO] [timer.py:181:stop] 0/1010, SamplesPerSec=3.8011496998307788
 51%|█████     | 1019/2013 [1:06:58<1:07:00,  4.04s/it][2022-01-08 02:46:26,488] [INFO] [logging.py:69:log_dist] [Rank 0] step=1020, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 51%|█████     | 1020/2013 [1:07:02<1:06:56,  4.04s/it][2022-01-08 02:46:26,517] [INFO] [timer.py:181:stop] 0/1020, SamplesPerSec=3.8002330853204738
 51%|█████     | 1029/2013 [1:07:38<1:06:21,  4.05s/it][2022-01-08 02:47:06,952] [INFO] [logging.py:69:log_dist] [Rank 0] step=1030, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:47:06,981] [INFO] [timer.py:181:stop] 0/1030, SamplesPerSec=3.799324762921159
 52%|█████▏    | 1039/2013 [1:08:19<1:05:41,  4.05s/it][2022-01-08 02:47:47,417] [INFO] [logging.py:69:log_dist] [Rank 0] step=1040, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 52%|█████▏    | 1040/2013 [1:08:23<1:05:37,  4.05s/it][2022-01-08 02:47:47,446] [INFO] [timer.py:181:stop] 0/1040, SamplesPerSec=3.798433009902687
 52%|█████▏    | 1049/2013 [1:08:59<1:05:00,  4.05s/it][2022-01-08 02:48:27,875] [INFO] [logging.py:69:log_dist] [Rank 0] step=1050, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 52%|█████▏    | 1050/2013 [1:09:03<1:04:55,  4.05s/it][2022-01-08 02:48:27,904] [INFO] [timer.py:181:stop] 0/1050, SamplesPerSec=3.7975656624936875
{'loss': 1.1015, 'learning_rate': 5e-05, 'epoch': 2.24}
 53%|█████▎    | 1059/2013 [1:09:39<1:04:20,  4.05s/it][2022-01-08 02:49:08,340] [INFO] [logging.py:69:log_dist] [Rank 0] step=1060, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:49:08,370] [INFO] [timer.py:181:stop] 0/1060, SamplesPerSec=3.796709676940089
 53%|█████▎    | 1069/2013 [1:10:20<1:03:39,  4.05s/it][2022-01-08 02:49:48,805] [INFO] [logging.py:69:log_dist] [Rank 0] step=1070, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 53%|█████▎    | 1070/2013 [1:10:24<1:03:35,  4.05s/it][2022-01-08 02:49:48,835] [INFO] [timer.py:181:stop] 0/1070, SamplesPerSec=3.7958689557314207
 54%|█████▎    | 1079/2013 [1:11:00<1:02:59,  4.05s/it][2022-01-08 02:50:29,285] [INFO] [logging.py:69:log_dist] [Rank 0] step=1080, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:50:29,315] [INFO] [timer.py:181:stop] 0/1080, SamplesPerSec=3.795030473812278
 54%|█████▍    | 1089/2013 [1:11:41<1:02:19,  4.05s/it][2022-01-08 02:51:09,752] [INFO] [logging.py:69:log_dist] [Rank 0] step=1090, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 54%|█████▍    | 1090/2013 [1:11:45<1:02:15,  4.05s/it][2022-01-08 02:51:09,782] [INFO] [timer.py:181:stop] 0/1090, SamplesPerSec=3.7942195484997527
 55%|█████▍    | 1099/2013 [1:12:21<1:01:38,  4.05s/it][2022-01-08 02:51:50,220] [INFO] [logging.py:69:log_dist] [Rank 0] step=1100, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:51:50,249] [INFO] [timer.py:181:stop] 0/1100, SamplesPerSec=3.7934240124344063
{'loss': 1.0865, 'learning_rate': 5e-05, 'epoch': 2.35}
 55%|█████▌    | 1109/2013 [1:13:02<1:00:57,  4.05s/it][2022-01-08 02:52:30,684] [INFO] [logging.py:69:log_dist] [Rank 0] step=1110, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:52:30,713] [INFO] [timer.py:181:stop] 0/1110, SamplesPerSec=3.7926460611632735
 56%|█████▌    | 1119/2013 [1:13:42<1:00:17,  4.05s/it][2022-01-08 02:53:11,152] [INFO] [logging.py:69:log_dist] [Rank 0] step=1120, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:53:11,182] [INFO] [timer.py:181:stop] 0/1120, SamplesPerSec=3.7918782335755292
 56%|█████▌    | 1129/2013 [1:14:23<59:37,  4.05s/it][2022-01-08 02:53:51,619] [INFO] [logging.py:69:log_dist] [Rank 0] step=1130, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:53:51,648] [INFO] [timer.py:181:stop] 0/1130, SamplesPerSec=3.791125838458544
 57%|█████▋    | 1139/2013 [1:15:03<58:57,  4.05s/it][2022-01-08 02:54:32,088] [INFO] [logging.py:69:log_dist] [Rank 0] step=1140, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:54:32,117] [INFO] [timer.py:181:stop] 0/1140, SamplesPerSec=3.790385068891586
 57%|█████▋    | 1149/2013 [1:15:44<58:16,  4.05s/it][2022-01-08 02:55:12,555] [INFO] [logging.py:69:log_dist] [Rank 0] step=1150, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 57%|█████▋    | 1150/2013 [1:15:48<58:12,  4.05s/it][2022-01-08 02:55:12,584] [INFO] [timer.py:181:stop] 0/1150, SamplesPerSec=3.789659158687227
{'loss': 1.0831, 'learning_rate': 5e-05, 'epoch': 2.46}
 58%|█████▊    | 1159/2013 [1:16:24<57:35,  4.05s/it][2022-01-08 02:55:53,020] [INFO] [logging.py:69:log_dist] [Rank 0] step=1160, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 58%|█████▊    | 1160/2013 [1:16:28<57:31,  4.05s/it][2022-01-08 02:55:53,050] [INFO] [timer.py:181:stop] 0/1160, SamplesPerSec=3.7889476647509777
 58%|█████▊    | 1169/2013 [1:17:05<56:53,  4.04s/it][2022-01-08 02:56:33,474] [INFO] [logging.py:69:log_dist] [Rank 0] step=1170, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 58%|█████▊    | 1170/2013 [1:17:09<56:49,  4.04s/it][2022-01-08 02:56:33,503] [INFO] [timer.py:181:stop] 0/1170, SamplesPerSec=3.7882581704339544
 59%|█████▊    | 1179/2013 [1:17:45<56:14,  4.05s/it][2022-01-08 02:57:13,934] [INFO] [logging.py:69:log_dist] [Rank 0] step=1180, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 59%|█████▊    | 1180/2013 [1:17:49<56:10,  4.05s/it][2022-01-08 02:57:13,964] [INFO] [timer.py:181:stop] 0/1180, SamplesPerSec=3.7875747382137805
 59%|█████▉    | 1189/2013 [1:18:26<55:33,  4.05s/it][2022-01-08 02:57:54,394] [INFO] [logging.py:69:log_dist] [Rank 0] step=1190, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 59%|█████▉    | 1190/2013 [1:18:30<55:29,  4.05s/it][2022-01-08 02:57:54,424] [INFO] [timer.py:181:stop] 0/1190, SamplesPerSec=3.7869031852927955
 60%|█████▉    | 1199/2013 [1:19:06<54:52,  4.05s/it][2022-01-08 02:58:34,851] [INFO] [logging.py:69:log_dist] [Rank 0] step=1200, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:58:34,880] [INFO] [timer.py:181:stop] 0/1200, SamplesPerSec=3.7862461317087472
{'loss': 1.0837, 'learning_rate': 5e-05, 'epoch': 2.56}
 60%|██████    | 1209/2013 [1:19:46<54:13,  4.05s/it][2022-01-08 02:59:15,319] [INFO] [logging.py:69:log_dist] [Rank 0] step=1210, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:59:15,349] [INFO] [timer.py:181:stop] 0/1210, SamplesPerSec=3.7855905492462787
 61%|██████    | 1219/2013 [1:20:27<53:32,  4.05s/it][2022-01-08 02:59:55,786] [INFO] [logging.py:69:log_dist] [Rank 0] step=1220, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 02:59:55,815] [INFO] [timer.py:181:stop] 0/1220, SamplesPerSec=3.7849475984423036
 61%|██████    | 1229/2013 [1:21:07<52:52,  4.05s/it][2022-01-08 03:00:36,249] [INFO] [logging.py:69:log_dist] [Rank 0] step=1230, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 61%|██████    | 1230/2013 [1:21:11<52:48,  4.05s/it][2022-01-08 03:00:36,280] [INFO] [timer.py:181:stop] 0/1230, SamplesPerSec=3.7843161726472756
 62%|██████▏   | 1239/2013 [1:21:48<52:11,  4.05s/it][2022-01-08 03:01:16,712] [INFO] [logging.py:69:log_dist] [Rank 0] step=1240, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:01:16,742] [INFO] [timer.py:181:stop] 0/1240, SamplesPerSec=3.7836979350673303
 62%|██████▏   | 1249/2013 [1:22:28<51:30,  4.05s/it][2022-01-08 03:01:57,170] [INFO] [logging.py:69:log_dist] [Rank 0] step=1250, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 62%|██████▏   | 1250/2013 [1:22:32<51:27,  4.05s/it][2022-01-08 03:01:57,199] [INFO] [timer.py:181:stop] 0/1250, SamplesPerSec=3.7830925652096035
{'loss': 1.0851, 'learning_rate': 5e-05, 'epoch': 2.67}
 63%|██████▎   | 1259/2013 [1:23:09<50:50,  4.05s/it][2022-01-08 03:02:37,631] [INFO] [logging.py:69:log_dist] [Rank 0] step=1260, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:02:37,660] [INFO] [timer.py:181:stop] 0/1260, SamplesPerSec=3.782495514881184
 63%|██████▎   | 1269/2013 [1:23:49<50:10,  4.05s/it][2022-01-08 03:03:18,092] [INFO] [logging.py:69:log_dist] [Rank 0] step=1270, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 63%|██████▎   | 1270/2013 [1:23:53<50:06,  4.05s/it][2022-01-08 03:03:18,121] [INFO] [timer.py:181:stop] 0/1270, SamplesPerSec=3.781907169837822
 64%|██████▎   | 1279/2013 [1:24:30<49:30,  4.05s/it][2022-01-08 03:03:58,558] [INFO] [logging.py:69:log_dist] [Rank 0] step=1280, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:03:58,587] [INFO] [timer.py:181:stop] 0/1280, SamplesPerSec=3.7813247189176713
 64%|██████▍   | 1289/2013 [1:25:10<48:49,  4.05s/it][2022-01-08 03:04:39,025] [INFO] [logging.py:69:log_dist] [Rank 0] step=1290, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 64%|██████▍   | 1290/2013 [1:25:14<48:45,  4.05s/it][2022-01-08 03:04:39,054] [INFO] [timer.py:181:stop] 0/1290, SamplesPerSec=3.780750847876627
 65%|██████▍   | 1299/2013 [1:25:51<48:08,  4.05s/it][2022-01-08 03:05:19,489] [INFO] [logging.py:69:log_dist] [Rank 0] step=1300, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 65%|██████▍   | 1300/2013 [1:25:55<48:04,  4.05s/it][2022-01-08 03:05:19,519] [INFO] [timer.py:181:stop] 0/1300, SamplesPerSec=3.780187448579763
{'loss': 1.1161, 'learning_rate': 5e-05, 'epoch': 2.78}
 65%|██████▌   | 1309/2013 [1:26:31<47:28,  4.05s/it][2022-01-08 03:05:59,952] [INFO] [logging.py:69:log_dist] [Rank 0] step=1310, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:05:59,982] [INFO] [timer.py:181:stop] 0/1310, SamplesPerSec=3.7796345096360033
 66%|██████▌   | 1319/2013 [1:27:12<46:48,  4.05s/it][2022-01-08 03:06:40,422] [INFO] [logging.py:69:log_dist] [Rank 0] step=1320, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 66%|██████▌   | 1320/2013 [1:27:16<46:44,  4.05s/it][2022-01-08 03:06:40,452] [INFO] [timer.py:181:stop] 0/1320, SamplesPerSec=3.779084920941463
 66%|██████▌   | 1329/2013 [1:27:52<46:08,  4.05s/it][2022-01-08 03:07:20,892] [INFO] [logging.py:69:log_dist] [Rank 0] step=1330, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:07:20,921] [INFO] [timer.py:181:stop] 0/1330, SamplesPerSec=3.778544062223671
 67%|██████▋   | 1339/2013 [1:28:32<45:28,  4.05s/it][2022-01-08 03:08:01,365] [INFO] [logging.py:69:log_dist] [Rank 0] step=1340, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 67%|██████▋   | 1340/2013 [1:28:37<45:23,  4.05s/it][2022-01-08 03:08:01,394] [INFO] [timer.py:181:stop] 0/1340, SamplesPerSec=3.7780095812402856
 67%|██████▋   | 1349/2013 [1:29:13<44:46,  4.05s/it][2022-01-08 03:08:41,829] [INFO] [logging.py:69:log_dist] [Rank 0] step=1350, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 67%|██████▋   | 1350/2013 [1:29:17<44:42,  4.05s/it][2022-01-08 03:08:41,858] [INFO] [timer.py:181:stop] 0/1350, SamplesPerSec=3.777488797080223
{'loss': 1.1174, 'learning_rate': 5e-05, 'epoch': 2.88}
 68%|██████▊   | 1359/2013 [1:29:53<44:06,  4.05s/it][2022-01-08 03:09:22,310] [INFO] [logging.py:69:log_dist] [Rank 0] step=1360, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:09:22,339] [INFO] [timer.py:181:stop] 0/1360, SamplesPerSec=3.77696479003716
 68%|██████▊   | 1369/2013 [1:30:34<43:27,  4.05s/it][2022-01-08 03:10:02,782] [INFO] [logging.py:69:log_dist] [Rank 0] step=1370, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:10:02,811] [INFO] [timer.py:181:stop] 0/1370, SamplesPerSec=3.7764540670625775
 69%|██████▊   | 1379/2013 [1:31:14<42:46,  4.05s/it][2022-01-08 03:10:43,257] [INFO] [logging.py:69:log_dist] [Rank 0] step=1380, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 69%|██████▊   | 1380/2013 [1:31:18<42:42,  4.05s/it][2022-01-08 03:10:43,286] [INFO] [timer.py:181:stop] 0/1380, SamplesPerSec=3.775948567947394
 69%|██████▉   | 1389/2013 [1:31:55<42:05,  4.05s/it][2022-01-08 03:11:23,726] [INFO] [logging.py:69:log_dist] [Rank 0] step=1390, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:11:23,756] [INFO] [timer.py:181:stop] 0/1390, SamplesPerSec=3.7754550081573566
 69%|██████▉   | 1399/2013 [1:32:35<41:25,  4.05s/it][2022-01-08 03:12:04,201] [INFO] [logging.py:69:log_dist] [Rank 0] step=1400, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 70%|██████▉   | 1400/2013 [1:32:39<41:20,  4.05s/it][2022-01-08 03:12:04,230] [INFO] [timer.py:181:stop] 0/1400, SamplesPerSec=3.774964902791683
{'loss': 1.1311, 'learning_rate': 5e-05, 'epoch': 2.99}
 70%|██████▉   | 1409/2013 [1:33:16<40:39,  4.04s/it][2022-01-08 03:12:44,503] [INFO] [logging.py:69:log_dist] [Rank 0] step=1410, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 70%|███████   | 1410/2013 [1:33:20<40:36,  4.04s/it][2022-01-08 03:12:44,532] [INFO] [timer.py:181:stop] 0/1410, SamplesPerSec=3.7745988851220726
 70%|███████   | 1419/2013 [1:33:56<40:03,  4.05s/it][2022-01-08 03:13:24,971] [INFO] [logging.py:69:log_dist] [Rank 0] step=1420, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:13:25,000] [INFO] [timer.py:181:stop] 0/1420, SamplesPerSec=3.7741260600756092
 71%|███████   | 1429/2013 [1:34:37<39:23,  4.05s/it][2022-01-08 03:14:05,447] [INFO] [logging.py:69:log_dist] [Rank 0] step=1430, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:14:05,477] [INFO] [timer.py:181:stop] 0/1430, SamplesPerSec=3.773655414403905
 71%|███████▏  | 1439/2013 [1:35:17<38:43,  4.05s/it][2022-01-08 03:14:45,923] [INFO] [logging.py:69:log_dist] [Rank 0] step=1440, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 72%|███████▏  | 1440/2013 [1:35:21<38:38,  4.05s/it][2022-01-08 03:14:45,953] [INFO] [timer.py:181:stop] 0/1440, SamplesPerSec=3.773191004457191
 72%|███████▏  | 1449/2013 [1:35:58<38:02,  4.05s/it][2022-01-08 03:15:26,398] [INFO] [logging.py:69:log_dist] [Rank 0] step=1450, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:15:26,427] [INFO] [timer.py:181:stop] 0/1450, SamplesPerSec=3.772734343995773
{'loss': 0.5773, 'learning_rate': 5e-05, 'epoch': 3.1}
 72%|███████▏  | 1459/2013 [1:36:38<37:22,  4.05s/it][2022-01-08 03:16:06,871] [INFO] [logging.py:69:log_dist] [Rank 0] step=1460, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:16:06,901] [INFO] [timer.py:181:stop] 0/1460, SamplesPerSec=3.772284399769869
 73%|███████▎  | 1469/2013 [1:37:18<36:41,  4.05s/it][2022-01-08 03:16:47,340] [INFO] [logging.py:69:log_dist] [Rank 0] step=1470, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:16:47,369] [INFO] [timer.py:181:stop] 0/1470, SamplesPerSec=3.7718439338319625
 73%|███████▎  | 1479/2013 [1:37:59<36:00,  4.05s/it][2022-01-08 03:17:27,810] [INFO] [logging.py:69:log_dist] [Rank 0] step=1480, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 74%|███████▎  | 1480/2013 [1:38:03<35:56,  4.05s/it][2022-01-08 03:17:27,839] [INFO] [timer.py:181:stop] 0/1480, SamplesPerSec=3.7714084083670345
 74%|███████▍  | 1489/2013 [1:38:39<35:20,  4.05s/it][2022-01-08 03:18:08,282] [INFO] [logging.py:69:log_dist] [Rank 0] step=1490, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:18:08,311] [INFO] [timer.py:181:stop] 0/1490, SamplesPerSec=3.7709779720469756
 74%|███████▍  | 1499/2013 [1:39:20<34:39,  4.04s/it][2022-01-08 03:18:48,742] [INFO] [logging.py:69:log_dist] [Rank 0] step=1500, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 75%|███████▍  | 1500/2013 [1:39:24<34:35,  4.05s/it][2022-01-08 03:18:48,771] [INFO] [timer.py:181:stop] 0/1500, SamplesPerSec=3.770560906286029
{'loss': 0.5255, 'learning_rate': 5e-05, 'epoch': 3.21}
 75%|███████▍  | 1509/2013 [1:40:00<33:59,  4.05s/it][2022-01-08 03:19:29,202] [INFO] [logging.py:69:log_dist] [Rank 0] step=1510, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:19:29,231] [INFO] [timer.py:181:stop] 0/1510, SamplesPerSec=3.7701494417930532
 75%|███████▌  | 1519/2013 [1:40:41<33:19,  4.05s/it][2022-01-08 03:20:09,667] [INFO] [logging.py:69:log_dist] [Rank 0] step=1520, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 76%|███████▌  | 1520/2013 [1:40:45<33:15,  4.05s/it][2022-01-08 03:20:09,696] [INFO] [timer.py:181:stop] 0/1520, SamplesPerSec=3.7697410296434013
 76%|███████▌  | 1529/2013 [1:41:21<32:38,  4.05s/it][2022-01-08 03:20:50,141] [INFO] [logging.py:69:log_dist] [Rank 0] step=1530, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 76%|███████▌  | 1530/2013 [1:41:25<32:34,  4.05s/it][2022-01-08 03:20:50,170] [INFO] [timer.py:181:stop] 0/1530, SamplesPerSec=3.769331455261203
 76%|███████▋  | 1539/2013 [1:42:02<31:58,  4.05s/it][2022-01-08 03:21:30,611] [INFO] [logging.py:69:log_dist] [Rank 0] step=1540, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:21:30,640] [INFO] [timer.py:181:stop] 0/1540, SamplesPerSec=3.7689303914076984
 77%|███████▋  | 1549/2013 [1:42:42<31:17,  4.05s/it][2022-01-08 03:22:11,082] [INFO] [logging.py:69:log_dist] [Rank 0] step=1550, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 77%|███████▋  | 1550/2013 [1:42:46<31:13,  4.05s/it][2022-01-08 03:22:11,112] [INFO] [timer.py:181:stop] 0/1550, SamplesPerSec=3.768533280844485
{'loss': 0.517, 'learning_rate': 5e-05, 'epoch': 3.31}
 77%|███████▋  | 1559/2013 [1:43:23<30:37,  4.05s/it][2022-01-08 03:22:51,545] [INFO] [logging.py:69:log_dist] [Rank 0] step=1560, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 77%|███████▋  | 1560/2013 [1:43:27<30:32,  4.05s/it][2022-01-08 03:22:51,574] [INFO] [timer.py:181:stop] 0/1560, SamplesPerSec=3.7681478650910862
 78%|███████▊  | 1569/2013 [1:44:03<29:57,  4.05s/it][2022-01-08 03:23:32,016] [INFO] [logging.py:69:log_dist] [Rank 0] step=1570, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:23:32,046] [INFO] [timer.py:181:stop] 0/1570, SamplesPerSec=3.7677608605512707
 78%|███████▊  | 1579/2013 [1:44:44<29:15,  4.05s/it][2022-01-08 03:24:12,479] [INFO] [logging.py:69:log_dist] [Rank 0] step=1580, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 78%|███████▊  | 1580/2013 [1:44:48<29:12,  4.05s/it][2022-01-08 03:24:12,509] [INFO] [timer.py:181:stop] 0/1580, SamplesPerSec=3.7673837218644195
 79%|███████▉  | 1589/2013 [1:45:24<28:35,  4.05s/it][2022-01-08 03:24:52,947] [INFO] [logging.py:69:log_dist] [Rank 0] step=1590, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 79%|███████▉  | 1590/2013 [1:45:28<28:31,  4.05s/it][2022-01-08 03:24:52,976] [INFO] [timer.py:181:stop] 0/1590, SamplesPerSec=3.767008802632748
 79%|███████▉  | 1599/2013 [1:46:05<27:55,  4.05s/it][2022-01-08 03:25:33,413] [INFO] [logging.py:69:log_dist] [Rank 0] step=1600, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:25:33,442] [INFO] [timer.py:181:stop] 0/1600, SamplesPerSec=3.7666400234585478
{'loss': 0.5351, 'learning_rate': 5e-05, 'epoch': 3.42}
 80%|███████▉  | 1609/2013 [1:46:45<27:14,  4.05s/it][2022-01-08 03:26:13,877] [INFO] [logging.py:69:log_dist] [Rank 0] step=1610, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 80%|███████▉  | 1610/2013 [1:46:49<27:10,  4.05s/it][2022-01-08 03:26:13,906] [INFO] [timer.py:181:stop] 0/1610, SamplesPerSec=3.7662768067385883
 80%|████████  | 1619/2013 [1:47:25<26:34,  4.05s/it][2022-01-08 03:26:54,343] [INFO] [logging.py:69:log_dist] [Rank 0] step=1620, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:26:54,372] [INFO] [timer.py:181:stop] 0/1620, SamplesPerSec=3.7659171247485923
 81%|████████  | 1629/2013 [1:48:06<25:53,  4.05s/it][2022-01-08 03:27:34,812] [INFO] [logging.py:69:log_dist] [Rank 0] step=1630, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 81%|████████  | 1630/2013 [1:48:10<25:49,  4.05s/it][2022-01-08 03:27:34,842] [INFO] [timer.py:181:stop] 0/1630, SamplesPerSec=3.7655598188243307
 81%|████████▏ | 1639/2013 [1:48:46<25:13,  4.05s/it][2022-01-08 03:28:15,282] [INFO] [logging.py:69:log_dist] [Rank 0] step=1640, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:28:15,311] [INFO] [timer.py:181:stop] 0/1640, SamplesPerSec=3.7652069853642303
 82%|████████▏ | 1649/2013 [1:49:27<24:33,  4.05s/it][2022-01-08 03:28:55,749] [INFO] [logging.py:69:log_dist] [Rank 0] step=1650, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 82%|████████▏ | 1650/2013 [1:49:31<24:28,  4.05s/it][2022-01-08 03:28:55,778] [INFO] [timer.py:181:stop] 0/1650, SamplesPerSec=3.7648604344072325
{'loss': 0.5438, 'learning_rate': 5e-05, 'epoch': 3.53}
 82%|████████▏ | 1659/2013 [1:50:07<23:52,  4.05s/it][2022-01-08 03:29:36,213] [INFO] [logging.py:69:log_dist] [Rank 0] step=1660, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:29:36,243] [INFO] [timer.py:181:stop] 0/1660, SamplesPerSec=3.7645192147909743
 83%|████████▎ | 1669/2013 [1:50:48<23:12,  4.05s/it][2022-01-08 03:30:16,679] [INFO] [logging.py:69:log_dist] [Rank 0] step=1670, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:30:16,709] [INFO] [timer.py:181:stop] 0/1670, SamplesPerSec=3.764180955869947
 83%|████████▎ | 1679/2013 [1:51:28<22:31,  4.05s/it][2022-01-08 03:30:57,144] [INFO] [logging.py:69:log_dist] [Rank 0] step=1680, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 83%|████████▎ | 1680/2013 [1:51:32<22:27,  4.05s/it][2022-01-08 03:30:57,173] [INFO] [timer.py:181:stop] 0/1680, SamplesPerSec=3.7638476574908815
 84%|████████▍ | 1689/2013 [1:52:09<21:51,  4.05s/it][2022-01-08 03:31:37,615] [INFO] [logging.py:69:log_dist] [Rank 0] step=1690, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:31:37,645] [INFO] [timer.py:181:stop] 0/1690, SamplesPerSec=3.7635141535703847
 84%|████████▍ | 1699/2013 [1:52:49<21:11,  4.05s/it][2022-01-08 03:32:18,093] [INFO] [logging.py:69:log_dist] [Rank 0] step=1700, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:32:18,122] [INFO] [timer.py:181:stop] 0/1700, SamplesPerSec=3.763182119064589
{'loss': 0.5523, 'learning_rate': 5e-05, 'epoch': 3.63}
 85%|████████▍ | 1709/2013 [1:53:30<20:30,  4.05s/it][2022-01-08 03:32:58,563] [INFO] [logging.py:69:log_dist] [Rank 0] step=1710, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:32:58,592] [INFO] [timer.py:181:stop] 0/1710, SamplesPerSec=3.7628579234650497
 85%|████████▌ | 1719/2013 [1:54:10<19:49,  4.05s/it][2022-01-08 03:33:39,036] [INFO] [logging.py:69:log_dist] [Rank 0] step=1720, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:33:39,065] [INFO] [timer.py:181:stop] 0/1720, SamplesPerSec=3.762535997206537
 86%|████████▌ | 1729/2013 [1:54:51<19:09,  4.05s/it][2022-01-08 03:34:19,497] [INFO] [logging.py:69:log_dist] [Rank 0] step=1730, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:34:19,527] [INFO] [timer.py:181:stop] 0/1730, SamplesPerSec=3.762223845502526
 86%|████████▋ | 1739/2013 [1:55:31<18:28,  4.05s/it][2022-01-08 03:34:59,961] [INFO] [logging.py:69:log_dist] [Rank 0] step=1740, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:34:59,990] [INFO] [timer.py:181:stop] 0/1740, SamplesPerSec=3.761914204829434
 87%|████████▋ | 1749/2013 [1:56:12<17:48,  4.05s/it][2022-01-08 03:35:40,431] [INFO] [logging.py:69:log_dist] [Rank 0] step=1750, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 87%|████████▋ | 1750/2013 [1:56:16<17:44,  4.05s/it][2022-01-08 03:35:40,460] [INFO] [timer.py:181:stop] 0/1750, SamplesPerSec=3.7616048576112933
{'loss': 0.5605, 'learning_rate': 5e-05, 'epoch': 3.74}
 87%|████████▋ | 1759/2013 [1:56:52<17:08,  4.05s/it][2022-01-08 03:36:20,905] [INFO] [logging.py:69:log_dist] [Rank 0] step=1760, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:36:20,935] [INFO] [timer.py:181:stop] 0/1760, SamplesPerSec=3.7612965120380717
 88%|████████▊ | 1769/2013 [1:57:33<16:27,  4.05s/it][2022-01-08 03:37:01,377] [INFO] [logging.py:69:log_dist] [Rank 0] step=1770, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:37:01,406] [INFO] [timer.py:181:stop] 0/1770, SamplesPerSec=3.7609931824089133
 88%|████████▊ | 1779/2013 [1:58:13<15:47,  4.05s/it][2022-01-08 03:37:41,845] [INFO] [logging.py:69:log_dist] [Rank 0] step=1780, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:37:41,874] [INFO] [timer.py:181:stop] 0/1780, SamplesPerSec=3.760695119732919
 89%|████████▉ | 1789/2013 [1:58:53<15:06,  4.05s/it][2022-01-08 03:38:22,320] [INFO] [logging.py:69:log_dist] [Rank 0] step=1790, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:38:22,349] [INFO] [timer.py:181:stop] 0/1790, SamplesPerSec=3.7603970137415414
 89%|████████▉ | 1799/2013 [1:59:34<14:26,  4.05s/it][2022-01-08 03:39:02,795] [INFO] [logging.py:69:log_dist] [Rank 0] step=1800, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 89%|████████▉ | 1800/2013 [1:59:38<14:21,  4.05s/it][2022-01-08 03:39:02,824] [INFO] [timer.py:181:stop] 0/1800, SamplesPerSec=3.760101944483825
{'loss': 0.5709, 'learning_rate': 5e-05, 'epoch': 3.85}
 90%|████████▉ | 1809/2013 [2:00:14<13:45,  4.05s/it][2022-01-08 03:39:43,270] [INFO] [logging.py:69:log_dist] [Rank 0] step=1810, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 90%|████████▉ | 1810/2013 [2:00:18<13:41,  4.05s/it][2022-01-08 03:39:43,299] [INFO] [timer.py:181:stop] 0/1810, SamplesPerSec=3.759810848057715
 90%|█████████ | 1819/2013 [2:00:55<13:05,  4.05s/it][2022-01-08 03:40:23,740] [INFO] [logging.py:69:log_dist] [Rank 0] step=1820, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 90%|█████████ | 1820/2013 [2:00:59<13:00,  4.05s/it][2022-01-08 03:40:23,769] [INFO] [timer.py:181:stop] 0/1820, SamplesPerSec=3.7595248648974176
 91%|█████████ | 1829/2013 [2:01:35<12:24,  4.05s/it][2022-01-08 03:41:04,207] [INFO] [logging.py:69:log_dist] [Rank 0] step=1830, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:41:04,236] [INFO] [timer.py:181:stop] 0/1830, SamplesPerSec=3.7592437188094854
 91%|█████████▏| 1839/2013 [2:02:16<11:44,  4.05s/it][2022-01-08 03:41:44,681] [INFO] [logging.py:69:log_dist] [Rank 0] step=1840, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:41:44,711] [INFO] [timer.py:181:stop] 0/1840, SamplesPerSec=3.7589622119151223
 92%|█████████▏| 1849/2013 [2:02:56<11:03,  4.05s/it][2022-01-08 03:42:25,155] [INFO] [logging.py:69:log_dist] [Rank 0] step=1850, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 92%|█████████▏| 1850/2013 [2:03:00<10:59,  4.05s/it][2022-01-08 03:42:25,184] [INFO] [timer.py:181:stop] 0/1850, SamplesPerSec=3.7586840886245376
{'loss': 0.5853, 'learning_rate': 5e-05, 'epoch': 3.95}
 92%|█████████▏| 1859/2013 [2:03:37<10:23,  4.05s/it][2022-01-08 03:43:05,629] [INFO] [logging.py:69:log_dist] [Rank 0] step=1860, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 92%|█████████▏| 1860/2013 [2:03:41<10:19,  4.05s/it][2022-01-08 03:43:05,659] [INFO] [timer.py:181:stop] 0/1860, SamplesPerSec=3.7584088253658683
 93%|█████████▎| 1869/2013 [2:04:17<09:42,  4.05s/it][2022-01-08 03:43:46,100] [INFO] [logging.py:69:log_dist] [Rank 0] step=1870, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 93%|█████████▎| 1870/2013 [2:04:21<09:38,  4.05s/it][2022-01-08 03:43:46,130] [INFO] [timer.py:181:stop] 0/1870, SamplesPerSec=3.7581376962393236
 93%|█████████▎| 1879/2013 [2:04:58<09:01,  4.04s/it][2022-01-08 03:44:26,386] [INFO] [logging.py:69:log_dist] [Rank 0] step=1880, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:44:26,415] [INFO] [timer.py:181:stop] 0/1880, SamplesPerSec=3.7579629767145355
 94%|█████████▍| 1889/2013 [2:05:38<08:21,  4.05s/it][2022-01-08 03:45:06,858] [INFO] [logging.py:69:log_dist] [Rank 0] step=1890, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:45:06,888] [INFO] [timer.py:181:stop] 0/1890, SamplesPerSec=3.757696318318126
 94%|█████████▍| 1899/2013 [2:06:18<07:41,  4.05s/it][2022-01-08 03:45:47,324] [INFO] [logging.py:69:log_dist] [Rank 0] step=1900, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 94%|█████████▍| 1900/2013 [2:06:22<07:37,  4.05s/it][2022-01-08 03:45:47,353] [INFO] [timer.py:181:stop] 0/1900, SamplesPerSec=3.75743633895728
{'loss': 0.4146, 'learning_rate': 5e-05, 'epoch': 4.06}
 95%|█████████▍| 1909/2013 [2:06:59<07:00,  4.05s/it][2022-01-08 03:46:27,788] [INFO] [logging.py:69:log_dist] [Rank 0] step=1910, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:46:27,817] [INFO] [timer.py:181:stop] 0/1910, SamplesPerSec=3.7571801921643524
 95%|█████████▌| 1919/2013 [2:07:39<06:20,  4.05s/it][2022-01-08 03:47:08,245] [INFO] [logging.py:69:log_dist] [Rank 0] step=1920, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 95%|█████████▌| 1920/2013 [2:07:43<06:16,  4.05s/it][2022-01-08 03:47:08,274] [INFO] [timer.py:181:stop] 0/1920, SamplesPerSec=3.7569294448435056
 96%|█████████▌| 1929/2013 [2:08:20<05:39,  4.05s/it][2022-01-08 03:47:48,710] [INFO] [logging.py:69:log_dist] [Rank 0] step=1930, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 96%|█████████▌| 1930/2013 [2:08:24<05:35,  4.05s/it][2022-01-08 03:47:48,740] [INFO] [timer.py:181:stop] 0/1930, SamplesPerSec=3.7566775980522458
 96%|█████████▋| 1939/2013 [2:09:00<04:59,  4.05s/it][2022-01-08 03:48:29,177] [INFO] [logging.py:69:log_dist] [Rank 0] step=1940, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 96%|█████████▋| 1940/2013 [2:09:04<04:55,  4.05s/it][2022-01-08 03:48:29,207] [INFO] [timer.py:181:stop] 0/1940, SamplesPerSec=3.7564275051520726
 97%|█████████▋| 1949/2013 [2:09:41<04:18,  4.05s/it][2022-01-08 03:49:09,639] [INFO] [logging.py:69:log_dist] [Rank 0] step=1950, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:49:09,668] [INFO] [timer.py:181:stop] 0/1950, SamplesPerSec=3.756182735788761
{'loss': 0.2756, 'learning_rate': 5e-05, 'epoch': 4.17}
 97%|█████████▋| 1959/2013 [2:10:21<03:38,  4.05s/it][2022-01-08 03:49:50,112] [INFO] [logging.py:69:log_dist] [Rank 0] step=1960, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 97%|█████████▋| 1960/2013 [2:10:25<03:34,  4.05s/it][2022-01-08 03:49:50,141] [INFO] [timer.py:181:stop] 0/1960, SamplesPerSec=3.75593523788703
 98%|█████████▊| 1969/2013 [2:11:02<02:58,  4.05s/it][2022-01-08 03:50:30,581] [INFO] [logging.py:69:log_dist] [Rank 0] step=1970, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 98%|█████████▊| 1970/2013 [2:11:06<02:53,  4.05s/it][2022-01-08 03:50:30,610] [INFO] [timer.py:181:stop] 0/1970, SamplesPerSec=3.755692055378648
 98%|█████████▊| 1979/2013 [2:11:42<02:17,  4.05s/it][2022-01-08 03:51:11,056] [INFO] [logging.py:69:log_dist] [Rank 0] step=1980, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:51:11,086] [INFO] [timer.py:181:stop] 0/1980, SamplesPerSec=3.7554480096485228
 99%|█████████▉| 1989/2013 [2:12:23<01:37,  4.05s/it][2022-01-08 03:51:51,528] [INFO] [logging.py:69:log_dist] [Rank 0] step=1990, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
 99%|█████████▉| 1990/2013 [2:12:27<01:33,  4.05s/it][2022-01-08 03:51:51,558] [INFO] [timer.py:181:stop] 0/1990, SamplesPerSec=3.7552084247483197
 99%|█████████▉| 1999/2013 [2:13:03<00:56,  4.05s/it][2022-01-08 03:52:32,004] [INFO] [logging.py:69:log_dist] [Rank 0] step=2000, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-01-08 03:52:32,033] [INFO] [timer.py:181:stop] 0/2000, SamplesPerSec=3.7549697349900724
{'loss': 0.2783, 'learning_rate': 5e-05, 'epoch': 4.27}
100%|█████████▉| 2009/2013 [2:13:44<00:16,  4.08s/it][2022-01-08 03:53:12,904] [INFO] [logging.py:69:log_dist] [Rank 0] step=2010, skipped=35, lr=[5e-05], mom=[[0.9, 0.999]]
100%|█████████▉| 2010/2013 [2:13:48<00:12,  4.08s/it][2022-01-08 03:53:12,942] [INFO] [timer.py:181:stop] 0/2010, SamplesPerSec=3.754532461339503
100%|██████████| 2013/2013 [2:14:00<00:00,  4.09s/it]
{'train_runtime': 8040.9292, 'train_samples_per_second': 3.748, 'train_steps_per_second': 0.25, 'train_loss': 1.4952462921673186, 'epoch': 4.3}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 2013/2013 [2:14:00<00:00,  3.99s/it]
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
0: While playing matchmaking games amid Argentine tradition crushing under the thumb of a father's radical ideologies, five students from divergent backgrounds find their paths crossing in this anthology film.
1: As a lonely woman awaits her high school girlfriend C quirks at the studentoves at the senior year, college separation becomes the most likely destination.
2: Hosts Sarah Thae-dies and Julian Mazzaro-Lopes gorge on pizza masters and masters of goblin Salem, Cesur, Tinker, and more in an all-new special.
3: Fowered by reuse chic urban style Gold artisaneer Gold embarks on an upscale quest for urban treasure in this reboot of the TV series Thwarted in the modern era.
4: From top-notch hangovers and rec center breakout Tomato, embark on comedic adventures across four countries in this Bollywood entertainment series.
5: An Afghanistan veteran's second attempt to kick-start his acting career while juggling a family affair puts him in the middle of a conflict involving political corruption, romantic rivalries and familiesubs.
6: Five hopeful friends journey south to meet royalty in the land of a royal consort and navigate crushing poverty in an area where development is impossible.
7: Amid the turbulent 1920s, five women launch a bold campaign against the man ordained to transform them into robotnic cab drivers. Each makes a perilous journey toward victory.
8: When the Irish MMA star gamer Colin Kennedy becomes sidelined by an accident, his political ambitions turn to private violence.
9: Join P. in his quest to find seven commonalities among humans living on Earth, to unearth the future of modern cults. Baking ventriloquist Syd Foster improvised characterizations for Pigskin parties in 12oos.
10: An off-color joke nearly got violent, triggering a deep obsession in an Argentine detective who’s involved with one victims of every cult at one peak. Reached epic — though comic Genoa Gray didn’t intend to get a big story.
11: A group of young college roomers gets caught up in a terrifying hook-up hell organized by a predatory vampire leader and cop Port nightclub gang Head vampire Squidgy. Faster episodes available on Netflix.
12: After waking up at a strange time in a barren town under a sky curses Rainbow city crewmember Aang Mok's weird adventures continue as he makes new friends and gets captured by an awful fate.
13: Amid last-minute fire in 1980 New York, the Neverending Que Delaps Mr. Fire Seguro and Fire Seguro, two rival shoe designers putting their visions for the webster sales strategy into action.
14: Trolls kidnap the king's daughters and overtake the kingdom of Messaba, desperate for affield with the monster Rani. Boon Pie becomes BFFs and BFFs fight for freedom.
15: Hosts Mike Huckle and Olivia Newton Oslo host the newest game show in which several players get a unique chance at victory. Host Emily Calandrelli explores the science behind life's many hidden riches.
16: From fangs to claws to venomous stings, it's no wonder how bad people manages to survive in the jungle, on the hunt for bad surprises.
17: Five individuals in 20th-century Korea find interconnected destinies as they pursue diverse path to happiness throughout the nation's Trojan horse Sen.
18: As communal violence erupts in his neighborhood, an undocumented immigrant set to return to Canada incurses the ire of two bitter rivals as internal war ignites.
19: When a father of two teen girls asks his students if they'd like sex, Mitsuba and Aotsuba Canary join host Mitsuhisa Neume to go out with these contestants.

Process finished with exit code 0